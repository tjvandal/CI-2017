{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Plan\n",
    "1. Synthetic Data - Generate data with Noise\n",
    "    i. Figure: 4 Plots - training_data, mc_dropout, concrete_dropout, mc_alpha  \n",
    "    ii. Figure: (x-axis) confidence interval vs (y-axis) percentage of samples in interval   \n",
    "2. Climate Downscaling Dataset (eval on test set)  \n",
    "    i. Table: RMSE, Corr, Prediction intervals  \n",
    "    ii. Figure: True vs Prediction, Distribution of samples for an extreme  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from models import ConcreteDropout, KerasMCDropout, smooth\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.patches import Polygon\n",
    "import seaborn as sns\n",
    "import scipy.stats \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "import keras.backend as K\n",
    "from keras import initializers, regularizers, optimizers\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Lambda, merge, Conv2D, MaxPooling2D, Flatten, Dropout, Wrapper\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "eps = 1e-8\n",
    "\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set some global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K_test = 5\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "default_dropout_rate = 0.5\n",
    "alpha = 0.5\n",
    "layer_sizes=[128,128,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC-Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout with Alpha-Divergence Loss\n",
    "\n",
    "### General Loss function\n",
    "$\\mathcal{L}(\\Theta) = \\dfrac{1}{\\alpha} \\sum_n \\text{log-sum-exp}\\big[ -\\alpha * l(y_n, f^{\\hat{\\omega}_k}(x_n)) \\big] + L_2(\\Theta)$\n",
    "\n",
    "### Regression Loss\n",
    "$l(y, f^{\\hat{\\omega}_k}(x_n)) = \\dfrac{1}{2}||y - f^{\\hat{\\omega}_k}(x)||_2^2$\n",
    "\n",
    "### Regression Alpha-Divergence Loss\n",
    "$\\mathcal{L}(\\Theta) = -\\dfrac{1}{\\alpha} \\sum_n \\text{log-sum-exp} \\big[ -\\dfrac{\\alpha\\tau}{2}||y - f^{\\hat{\\omega}_k}(x)||_2^2 \\big] + \\dfrac{ND}{2}\\text{log}\\tau + \\sum_i p_i ||M_i||_2^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate Downscaling Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPQAAAD3CAYAAAAqu3lQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d79f290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPQAAAD3CAYAAAAqu3lQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naCKN/C9pNz/14FBjdAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f591110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "Kmc = 30\n",
    "\n",
    "def read_file(f):\n",
    "    data = pd.read_csv(f, header=None)\n",
    "    data = data.rename(columns={0: 'time', 1: 'label'})\n",
    "    for c in data.columns:\n",
    "        if isinstance(c, int) and ((c % 2) == 1):\n",
    "            del data[c]\n",
    "    X = data.values[:,2:]\n",
    "    X = X.reshape((X.shape[0], 9, 9))[:,:,:,np.newaxis]\n",
    "    plt.imshow(X[0,:,:,0], interpolation='nearest')\n",
    "    plt.show()\n",
    "    y = data.values[:,1,np.newaxis]\n",
    "    return X, y\n",
    "\n",
    "train_file = 'data/train_28.5384_-81.3792'\n",
    "test_file = 'data/test_28.5384_-81.3792'\n",
    "\n",
    "X_train, Y_train = read_file(train_file)\n",
    "X_test, Y_test = read_file(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Concrete Dropout\n",
    "\n",
    "$P(X=x) = P(X>0) P(X | X>0) + P(X=0)$  \n",
    "$P(X=x) = P(X>0) P(X | X>0) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def normal_loss(true, pred):\n",
    "    mean = pred[:, :1]\n",
    "    log_var = pred[:, 1:]         ## log(sigma**2)\n",
    "    precision = K.exp(-log_var)   ## 1/sigma**2\n",
    "    return K.sum(precision * (true - mean)**2. + log_var, -1) \n",
    "\n",
    "def lognormal_loss(true, pred):\n",
    "    mean = pred[:, :1]\n",
    "    log_var = pred[:, 1:]         ## log(sigma**2)\n",
    "    precision = K.exp(-log_var)   ## 1/sigma**2\n",
    "    return K.sum(precision * (tf.log(true) - mean)**2. + log_var, -1)  ## log(x) for the log distribution\n",
    "\n",
    "def gamma_loss(true, pred):\n",
    "    alpha = K.exp(pred[:, :1])\n",
    "    beta = K.exp(pred[:, 1:])\n",
    "    return -(alpha - 1) * K.sum(K.log(true)) + beta * K.sum(true)\n",
    "\n",
    "def gumbel_loss(true, pred):\n",
    "    mu = pred[:,:1]\n",
    "    beta = K.exp(pred[:,1:])\n",
    "    l = K.sum((true - mu) / beta) + batch_size * K.log(beta)\n",
    "    l += K.sum(K.exp(-(true - mu)/beta))\n",
    "    return l\n",
    "\n",
    "def fit_concrete_climate(nb_epoch, X, Y, output_activation=None, verbose=0,\n",
    "                        layer_sizes=[128,128], distribution='normal'):\n",
    "\n",
    "    if K.backend() == 'tensorflow':\n",
    "        K.clear_session()\n",
    "    N = X.shape[0]\n",
    "    wd = lr**2. / N\n",
    "    dd = 2. / N\n",
    "    inp = Input(shape=X.shape[1:])\n",
    "    x = inp\n",
    "    for h in layer_sizes:\n",
    "        x = ConcreteDropout(Conv2D(h, (3,3), padding='valid', activation='relu'), \n",
    "                            weight_regularizer=wd, \n",
    "                            dropout_regularizer=dd)(x)\n",
    "    #x = K.reshape(x, (-1, 9*9))\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    mean = ConcreteDropout(Dense(1, activation=output_activation), \n",
    "                           weight_regularizer=wd, dropout_regularizer=dd)(x)\n",
    "    log_var = ConcreteDropout(Dense(1), weight_regularizer=wd, dropout_regularizer=dd)(x)\n",
    "    out = merge([mean, log_var], mode='concat')\n",
    "    model = Model(inp, out)\n",
    "    \n",
    "    loss = normal_loss\n",
    "    \n",
    "    #adam = optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, \n",
    "    #                       epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss=loss, optimizer='adam')\n",
    "\n",
    "    hist = model.fit(X, Y, epochs=nb_epoch, batch_size=batch_size, verbose=verbose)\n",
    "    Y_est = model.predict(X)\n",
    "    hist_loss = hist.history['loss']\n",
    "    return model, hist_loss  # return ELBO up to const.\n",
    "\n",
    "def normal_moments(samples):\n",
    "    \"\"\"\n",
    "    Prior distribution is normal\n",
    "    E[X] = mu\n",
    "    Var[X] = sigma**2\n",
    "    \"\"\"\n",
    "    mu = samples[:,0].mean(axis=1)\n",
    "    second_moment = np.zeros(samples.shape[0])\n",
    "    for k in range(samples.shape[2]):\n",
    "        sample_var = np.exp(samples[:,1,k])\n",
    "        sample_mean = samples[:,0,k]\n",
    "        second_moment += sample_var + mu**2\n",
    "    \n",
    "    sigma2 = second_moment / samples.shape[2] - mu**2\n",
    "    #Y_mean = np.exp(mu + sigma2 / 2.)\n",
    "    #Y_var = (np.exp(sigma2) - 1) * np.exp(2*mu + sigma2)\n",
    "    return mu, sigma2\n",
    "\n",
    "def gumbel_moments(samples):\n",
    "    '''\n",
    "    E[X] = mu + beta * 0.5772\n",
    "    Var[X] = pi**2 beta**2 / 6\n",
    "    '''\n",
    "    euler_constant = 0.5772156649\n",
    "    alpha_samples = samples[:,0]\n",
    "    beta_samples = np.exp(samples[:,1])\n",
    "        \n",
    "    alpha_mean = alpha_samples.mean(axis=1)\n",
    "    beta_mean = beta_samples.mean(axis=1)\n",
    "    \n",
    "    sample_mean = alpha_samples + beta_samples * euler_constant\n",
    "    sample_var = np.pi**2 * beta_samples**2 / 6.\n",
    "        \n",
    "    EX = sample_mean.mean(axis=1)\n",
    "    EX2 = (sample_var + EX[:,np.newaxis]**2).mean(axis=1)\n",
    "    VX = EX2 - EX**2\n",
    "    \n",
    "    beta_hat = (VX * 6. / (np.pi**2))**0.5\n",
    "    alpha_hat = EX - beta_hat * euler_constant\n",
    "    \n",
    "    return alpha_hat, beta_hat\n",
    "\n",
    "def heteroskedastic_loss(true, pred):\n",
    "    mean = pred[:,:1]\n",
    "    log_var = pred[:,1:]\n",
    "    precision = K.exp(-log_var)\n",
    "    return K.sum(precision * (true - mean)**2. + log_var)\n",
    "\n",
    "def fit_mc(nb_epoch, X, Y, output_activation=None, verbose=0,\n",
    "           alpha_divergence=False, dropout_prob=default_dropout_rate,\n",
    "          layer_sizes=layer_sizes):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        K.clear_session()\n",
    "    N = X.shape[0]\n",
    "    D = np.prod(X.shape[1:])\n",
    "    inp = Input(shape=X.shape[1:])\n",
    "    x = inp\n",
    "    #default_dropout_rate = 0.1\n",
    "    for h in layer_sizes:\n",
    "        x = KerasMCDropout(Conv2D(h, (3,3), padding='valid', activation='relu'), \n",
    "                           dropout_prob=dropout_prob)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    mean = KerasMCDropout(Dense(1, activation=output_activation), dropout_prob=dropout_prob)(x)\n",
    "    log_var = KerasMCDropout(Dense(1), dropout_prob=dropout_prob)(x)\n",
    "    out = merge([mean, log_var], mode='concat')\n",
    "        \n",
    "    model = Model(inp, out)\n",
    "    adam = optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0)\n",
    "\n",
    "    if alpha_divergence:\n",
    "        model.compile(optimizer='adam', loss=alpha_divergence_loss)\n",
    "    else:\n",
    "        model.compile(optimizer='adam', loss=heteroskedastic_loss)\n",
    "\n",
    "    hist = model.fit(X, Y, epochs=nb_epoch, batch_size=batch_size, verbose=verbose)\n",
    "    loss = hist.history['loss']\n",
    "    return model, loss  # return ELBO up to const.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Kmc=10\n",
    "def alpha_divergence_loss(true, preds, alpha=alpha, D=1., Kmc=Kmc):\n",
    "    # true: shape (N,K,1)\n",
    "    # preds of shape (N,K,2)\n",
    "    # heteroskedastic case\n",
    "    N = preds.shape[0]\n",
    "    true = true[:,:,0]  # (N,K)\n",
    "    mean_samples = preds[:,:,0]     # (N,K)\n",
    "    log_var_samples = preds[:,:,1]  # (N,K)\n",
    "    precision = K.exp(-log_var_samples) # (N,K)\n",
    "    \n",
    "   # ll = precision/2.*(true - mean_samples)**2. + log_var_samples\n",
    "    ll = precision *(true - mean_samples)**2. + log_var_samples\n",
    "    ll = K.sum(K.exp(-alpha * ll), axis=-1) # (N,)\n",
    "    l = -1/alpha*K.sum(K.log(ll+1e-8))    # float\n",
    "    #l += K.sum(K.log(precision)) * D / 2.\n",
    "    return l\n",
    "    \n",
    "def split_tensor(x):\n",
    "    x = [K.expand_dims(_x, 1) for _x in tf.split(x, Kmc, 0)]\n",
    "    return layers.concatenate(x, axis=1)\n",
    "    \n",
    "def fit_alpha(nb_epoch, X, Y, output_activation=None, verbose=0,\n",
    "           alpha_divergence=False, dropout_prob=default_dropout_rate,\n",
    "          layer_sizes=layer_sizes, Kmc=Kmc):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        K.clear_session()\n",
    "    N = X.shape[0]\n",
    "    D = np.prod(X.shape[1:])\n",
    "    inp = Input(shape=X.shape[1:])\n",
    "    x = inp\n",
    "    x = layers.concatenate([x for _ in range(Kmc)], axis=0)\n",
    "    for h in layer_sizes:\n",
    "        x = KerasMCDropout(Conv2D(h, (3,3), padding='valid', activation='relu'), \n",
    "                           dropout_prob=dropout_prob)(x)\n",
    "    x = Flatten()(x)\n",
    "    mean = KerasMCDropout(Dense(1, activation=output_activation), dropout_prob=dropout_prob)(x)\n",
    "    log_var = KerasMCDropout(Dense(1), dropout_prob=dropout_prob)(x)\n",
    "    out = merge([mean, log_var], mode='concat')\n",
    "\n",
    "    out = layers.Lambda(split_tensor)(out)\n",
    "    model = Model(inp, out)\n",
    "    \n",
    "    #adam = optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0)\n",
    "    #sgd = optimizers.SGD(lr=lr, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=alpha_divergence_loss)\n",
    "\n",
    "    hist = model.fit(X, Y, epochs=nb_epoch, batch_size=batch_size, verbose=verbose)\n",
    "    loss = hist.history['loss']\n",
    "    return model, loss  # return ELBO up to const.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tj/my-env/lib/python2.7/site-packages/ipykernel_launcher.py:39: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8766/8766 [==============================] - 7s - loss: -776.8491     \n",
      "Epoch 2/20\n",
      "8766/8766 [==============================] - 6s - loss: -898.7382     \n",
      "Epoch 3/20\n",
      "8766/8766 [==============================] - 6s - loss: -933.4898     \n",
      "Epoch 4/20\n",
      "8766/8766 [==============================] - 6s - loss: -969.1976     \n",
      "Epoch 5/20\n",
      "8766/8766 [==============================] - 7s - loss: -962.4050     \n",
      "Epoch 6/20\n",
      "8766/8766 [==============================] - 7s - loss: -993.8752     \n",
      "Epoch 7/20\n",
      "8766/8766 [==============================] - 7s - loss: -1013.3945     \n",
      "Epoch 8/20\n",
      "8766/8766 [==============================] - 6s - loss: -1019.1166     \n",
      "Epoch 9/20\n",
      "8766/8766 [==============================] - 6s - loss: -1032.8403     \n",
      "Epoch 10/20\n",
      "8766/8766 [==============================] - 6s - loss: -1034.1827     \n",
      "Epoch 11/20\n",
      "8766/8766 [==============================] - 6s - loss: -1013.1956     \n",
      "Epoch 12/20\n",
      "8766/8766 [==============================] - 6s - loss: -1035.8140     \n",
      "Epoch 13/20\n",
      "8766/8766 [==============================] - 6s - loss: -1052.0439     \n",
      "Epoch 14/20\n",
      "8766/8766 [==============================] - 7s - loss: -997.3928      \n",
      "Epoch 15/20\n",
      "8766/8766 [==============================] - 7s - loss: -997.8353      \n",
      "Epoch 16/20\n",
      "8766/8766 [==============================] - 7s - loss: -1021.8832     \n",
      "Epoch 17/20\n",
      "8766/8766 [==============================] - 7s - loss: -1036.0698     \n",
      "Epoch 18/20\n",
      "8766/8766 [==============================] - 6s - loss: -1038.6993     \n",
      "Epoch 19/20\n",
      "8766/8766 [==============================] - 6s - loss: -1035.7787     \n",
      "Epoch 20/20\n",
      "8766/8766 [==============================] - 6s - loss: -1039.2432     \n",
      "(3287, 2, 30)\n"
     ]
    }
   ],
   "source": [
    "distribution = 'normal'\n",
    "\n",
    "train_rainy_days = X_train.mean(axis=3).mean(axis=2).mean(axis=1) > 0.0\n",
    "test_rainy_days = X_test.mean(axis=3).mean(axis=2).mean(axis=1) > 0.0\n",
    "\n",
    "x_mu = np.mean(X_train[train_rainy_days])\n",
    "x_std = np.std(X_train[train_rainy_days])\n",
    "X_train_norm = (X_train - x_mu) / x_std\n",
    "X_test_norm = (X_test - x_mu) / x_std\n",
    "\n",
    "Y_train[train_rainy_days] += eps\n",
    "Y_test[test_rainy_days] += eps\n",
    "\n",
    "scale_y = StandardScaler().fit(Y_train[train_rainy_days])\n",
    "Y_train_norm = scale_y.transform(Y_train)\n",
    "\n",
    "epochs = 20\n",
    "layer_sizes=[64,]\n",
    "# Train Concrete\n",
    "\n",
    "get_samples = lambda model: np.array([model.predict(X_test_norm) for _ in range(Kmc)]).swapaxes(0,1).swapaxes(1,2)\n",
    "\n",
    "samples = dict()\n",
    "\n",
    "load = False\n",
    "if load:\n",
    "    mc_model = load_model('mc-model.h5', custom_objects={'KerasMCDropout': KerasMCDropout, \n",
    "                                                'heteroskedastic_loss': heteroskedastic_loss})\n",
    "    samples['mc'] = get_samples(mc_model)\n",
    "    concrete_model = load_model('concrete-model.h5', custom_objects={'ConcreteDropout': ConcreteDropout,\n",
    "                                                                    'normal_loss': normal_loss})\n",
    "    samples['concrete'] = get_samples(concrete_model)\n",
    "    #alpha_model = load_model('alpha-model.h5', custom_objects={'KerasMCDropout': KerasMCDropout,\n",
    "    #                                                          'alpha_divergence_loss': alpha_divergence_loss})\n",
    "    #samples['alpha'] = get_samples(alpha_model)\n",
    "    #alpha_samples = alpha_model.predict(X_test_norm).swapaxes(1,2)\n",
    "else: \n",
    "    \n",
    "    # Train Models\n",
    "    '''\n",
    "    concrete_model, loss = fit_concrete_climate(epochs, X_train_norm, Y_train_norm, \n",
    "                output_activation=None, verbose=1, layer_sizes=layer_sizes, \n",
    "                distribution=distribution)\n",
    "    concrete_model.save('concrete-model.h5')\n",
    "    samples['concrete'] = get_samples(concrete_model)\n",
    "\n",
    "    mc_model, loss = fit_mc(epochs, X_train_norm, Y_train_norm, alpha_divergence=False,\n",
    "                output_activation=None, verbose=1, layer_sizes=layer_sizes)\n",
    "    mc_model.save('mc-model.h5')\n",
    "    samples['mc'] = get_samples(mc_model)\n",
    "    '''\n",
    "    Y_train_norm = Y_train_norm[:,:,np.newaxis]\n",
    "    Y_train_norm_stack = np.concatenate([Y_train_norm for _ in range(Kmc)], axis=1).astype(np.float32)\n",
    "    alpha_model, loss = fit_alpha(epochs, X_train_norm.astype(np.float32), Y_train_norm_stack, alpha_divergence=True,\n",
    "                output_activation=None, verbose=1, layer_sizes=layer_sizes)\n",
    "    alpha_model.save('alpha-model.h5')\n",
    "    #alpha_samples = get_samples(alpha_model)\n",
    "    samples['alpha'] = np.concatenate([alpha_model.predict(X_test_norm).swapaxes(1,2) for _ in range(3)], axis=2)\n",
    "    print samples['alpha'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How good is our coverage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "## Keep this for later\n",
    "#if distribution == 'gumbel':\n",
    "#    alpha, beta = gumbel_moments(Y_samples[test_rainy_days])\n",
    "#    mu = alpha + beta * 0.5772\n",
    "#    sigma2 = np.pi**2 / 6. * beta**2\n",
    "\n",
    "def get_mean_var(samples):\n",
    "    mu, sigma2 = normal_moments(samples)\n",
    "    mu = mu * scale_y.var_**0.5 + scale_y.mean_\n",
    "    sigma2 *= scale_y.var_\n",
    "    return mu, sigma2\n",
    "\n",
    "#concrete_mu, concrete_sigma2 = get_mean_var(samples['concrete'][test_rainy_days])\n",
    "#mc_mu, mc_sigma2 = get_mean_var(samples['mc'][test_rainy_days])\n",
    "alpha_mu, alpha_sigma2 = get_mean_var(samples['alpha'][test_rainy_days])\n",
    "\n",
    "get_rmse = lambda x: np.mean((x -  Y_test[test_rainy_days, 0])**2)**0.50\n",
    "\n",
    "#concrete_rmse = get_rmse(concrete_mu)\n",
    "alpha_rmse = get_rmse(alpha_mu)\n",
    "#mc_rmse = get_rmse(mc_mu)\n",
    "print \"RMSE -- ALpha: %2.2f\" % (alpha_rmse)\n",
    "print \"RMSE -- Concrete: %2.2f, MC: %2.2f\" % (concrete_rmse, mc_rmse)\n",
    "\n",
    "plt.hist(concrete_sigma2**0.5, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def coverage_plot(samples, y, ax, name=None):\n",
    "    mu, sigma2 = get_mean_var(samples)\n",
    "    sigma = np.sqrt(sigma2)\n",
    "    p_range = np.arange(0.0, 1.0, 0.01)\n",
    "    ratios = []\n",
    "    std = sigma2**0.5\n",
    "\n",
    "    normal_dist = scipy.stats.norm(0,1)\n",
    "    for p_interval in p_range:\n",
    "        p = 0.5 + p_interval/2.\n",
    "        plow = (1. - p)/2\n",
    "        phi = 1-plow\n",
    "        \n",
    "        zscale = normal_dist.ppf(p)\n",
    "        sample_hi = mu + zscale * std\n",
    "        sample_low = mu - zscale * std\n",
    "        \n",
    "        ratios.append(np.mean((sample_hi > y) & (y > sample_low)))\n",
    "            \n",
    "    err = np.mean((p_range - np.array(ratios))**2)**0.5\n",
    "    ax.plot(p_range, ratios, label='%s - Freq RMSE: %2.2f' % (name.upper(), err))\n",
    "    \n",
    "fig, axs = plt.subplots(1,2,figsize=(12,5))\n",
    "axs = np.ravel(axs)\n",
    "\n",
    "for key in sorted(samples.keys()):\n",
    "    coverage_plot(samples[key][test_rainy_days], Y_test[test_rainy_days], axs[0], name=key)\n",
    "    #coverage_plot(samples[key], Y_test, axs[0], name=key)\n",
    "\n",
    "axs[0].plot([0,1],[0,1], ls=':')\n",
    "axs[0].legend()\n",
    "axs[0].set_title(\"Uncertainty Calibration\")\n",
    "axs[0].set_ylabel(\"Frequency\")\n",
    "axs[0].set_xlabel(\"Probability\")\n",
    "axs[0].text(-0.1,1.05, 'A)', horizontalalignment='center', verticalalignment='center',\n",
    "                transform=axs[0].transAxes, fontsize=14)\n",
    "axs[1].bar([1,2], [mc_rmse, concrete_rmse])\n",
    "axs[1].set_ylim([5.5,6.5])\n",
    "axs[1].set_xticks([1,2])\n",
    "axs[1].set_xticklabels(sorted(samples.keys()))\n",
    "axs[1].set_title(\"Predictive Performance\")\n",
    "axs[1].set_ylabel(\"RMSE (mm/day)\")\n",
    "axs[1].text(-0.1,1.05, 'B)', horizontalalignment='center', verticalalignment='center',\n",
    "                transform=axs[1].transAxes, fontsize=14)\n",
    "plt.savefig(\"uq-results.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END of Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = np.argsort(rainy_test)[-100]\n",
    "\n",
    "print rainy_test[idx]\n",
    "print X_test[test_rainy_days][idx].mean()\n",
    "\n",
    "m = mu[idx-5:idx+6]\n",
    "s = np.sqrt(sigma2[idx-5:idx+6])\n",
    "\n",
    "\n",
    "y_est = []\n",
    "low, high = [], []\n",
    "for i in range(len(m)):\n",
    "    if distribution == 'normal':\n",
    "        dist = scipy.stats.norm(loc=m[i], scale=np.sqrt(s[i]))\n",
    "    elif distribution == 'gumbel':\n",
    "        dist = scipy.stats.gumbel_r(loc=alpha[idx-5:idx+6][i], \n",
    "                                    scale=beta[idx-5:idx+6][i])\n",
    "    y_est += [dist.mean()]\n",
    "    high += [dist.ppf(0.75)]\n",
    "    low += [dist.ppf(0.25)]\n",
    "\n",
    "label = rainy_test[idx-5:idx+6]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(y_est)\n",
    "plt.plot(label, ls=':')\n",
    "plt.plot(low, color='green', ls='--')\n",
    "plt.plot(high, color='red', ls='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdf = []\n",
    "for i in np.where(rainy_test > 0.)[0]:\n",
    "    if distribution == 'gumbel':\n",
    "        dist = scipy.stats.gumbel_r(loc=alpha[i],scale=beta[i])\n",
    "    elif distribution == 'normal':\n",
    "        dist = scipy.stats.norm(loc=mu[i], scale=sigma2[i]**(0.5))\n",
    "    cdf.append(dist.cdf(Y_test[i])[0])\n",
    "    \n",
    "plt.hist(cdf)\n",
    "plt.xlabel(\"CDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
